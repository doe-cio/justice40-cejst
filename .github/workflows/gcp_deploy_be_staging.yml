name: GCP Deploy Backend Staging
on:
  pull_request:
    branches:
      - main
      - "**/release/**"
    paths:
      - "data/**"

  workflow_dispatch:
    branches:
      - gcp_workflows
    paths:
      - "data/**/"
env:
  ###PR_NUMBER: ${{github.event.pull_request.number}}
  PR_NUMBER: 12345 
  ###SHA_NUMBER: ${{github.event.pull_request.head.sha}}
  SHA_NUMBER: 123 
  CENSUS_API_KEY: ${{ secrets.CENSUS_API_KEY }}
jobs:
  generate-score-tiles:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: data/data-pipeline
    strategy:
      matrix:
        python-version: [3.9]

    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
      - name: Checkout source
        uses: actions/checkout@v2
      - name: Print variables to help debug
        uses: hmarr/debug-action@v2
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - run: pip install -U wheel
      - name: Load cached Poetry installation
        id: cached-poetry-dependencies
        uses: actions/cache@v2
        with:
          path: ~/.cache/pypoetry/virtualenvs
          key: env-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/poetry.lock') }}-${{ hashFiles('.github/workflows/gcp_deploy_be_staging.yml') }}
      - name: Install poetry
        uses: snok/install-poetry@v1.3.3
      - name: Print Poetry settings
        run: poetry show -v
      - name: Install GDAL/ogr2ogr
        run: |
          sudo apt-get update
          sudo apt-get -y install gdal-bin
          ogrinfo --version
      - name: Install dependencies
        run: poetry add s4cmd && poetry install
        if: steps.cached-poetry-dependencies.outputs.cache-hit != 'true'

      - id: 'auth'
        uses: 'google-github-actions/auth@v1'
        with:
          workload_identity_provider: 'projects/1059665175923/locations/global/workloadIdentityPools/j40-pool/providers/j40-provider'
          service_account: 'j40-service-account@prj-icc-sandbox-poc.iam.gserviceaccount.com'

      - name: Download census geo data for later user
        run: |
          poetry run python3 data_pipeline/application.py pull-census-data -s aws
      - name: Generate Score
        run: |
          poetry run python3 data_pipeline/application.py score-full-run
      - name: Generate Score Post
        run: |
          poetry run python3 data_pipeline/application.py generate-score-post
      - name: Generate Score Geo
        run: |
          poetry run python3 data_pipeline/application.py geo-score
      - name: Run Smoketests
        run: |
          poetry run pytest data_pipeline/ -m smoketest
      ###- name: Deploy Score to GCP 
      ##  run: |
      ###   poetry run s4cmd put ./data_pipeline/data/score/csv/ s3://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/csv --recursive --force --API-ACL=public-read
      ###    poetry run s4cmd put ./data_pipeline/files/ s3://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/downloadable --recursive --force --API-ACL=public-read

      - id: 'upload-files'
        uses: 'google-github-actions/upload-cloud-storage@v1'
        with:
          path: data/data-pipeline/data_pipeline/data/score/csv/ 
          destination: 'justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/csv'
          parent: false

      - id: 'upload-files2'
        uses: 'google-github-actions/upload-cloud-storage@v1'
        with:
          path: data/data-pipeline/data_pipeline/files/
          destination: 'justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/downloadable'
          parent: false

      - name: Update PR with deployed Score URLs
        uses: mshick/add-pr-comment@v1
        with:
          # Deploy to S3 for the Staging URL
          message: |
            ** Score Deployed! **
            Find it here:
            ###- Score Full usa.csv: https://justice40-data.s3.amazonaws.com/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/csv/full/usa.csv
            ###- Download Zip Packet: https://justice40-data.s3.amazonaws.com/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/downloadable/Screening_Tool_Data.zip
            
            - Score Full usa.csv: https://storage.googleapis.com/justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/csv/full/usa.csv
            - Download Zip Packet: https://storage.googleapis.com/justice40/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/downloadable/Screening_Tool_Data.zip
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          repo-token-user-login: "github-actions[bot]"
          allow-repeats: false
      - name: Set timezone for tippecanoe
        uses: szenius/set-timezone@v1.0
        with:
          timezoneLinux: "America/Los_Angeles"
      - name: Get tippecanoe
        run: |
          sudo apt-get install -y software-properties-common libsqlite3-dev zlib1g-dev
          sudo apt-add-repository -y ppa:git-core/ppa
          sudo mkdir -p /tmp/tippecanoe-src
          sudo git clone https://github.com/mapbox/tippecanoe.git /tmp/tippecanoe-src
      - name: Make tippecanoe
        working-directory: /tmp/tippecanoe-src
        run: |
          sudo /usr/bin/bash -c make
          mkdir -p /usr/local/bin
          cp tippecanoe /usr/local/bin/tippecanoe
          tippecanoe -v
      - name: Generate Tiles
        run: |
          poetry run python3 data_pipeline/application.py generate-map-tiles
      - name: Deploy Map to Geoplatform AWS
        run: |
          poetry run s4cmd put ./data_pipeline/data/score/geojson/ gs://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/geojson --recursive --force  --API-ACL=public-read --num-threads=250
          poetry run s4cmd put ./data_pipeline/data/score/shapefile/ gs://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/shapefile --recursive --force  --API-ACL=public-read
          poetry run s4cmd put ./data_pipeline/data/score/tiles/ gs://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/tiles --recursive --force  --API-ACL=public-read --num-threads=250
          poetry run s4cmd put ./data_pipeline/data/score/downloadable/ gs://justice40-data/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/downloadable --recursive --force --API-ACL=public-read
      - name: Update PR with deployed Map URL
        uses: mshick/add-pr-comment@v1
        with:
          # Deploy to S3 for the staging URL
          message: |
            ** Map Deployed! **
            Map with Staging Backend: https://screeningtool.geoplatform.gov/en?flags=stage_hash=${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}
            Find tiles here: https://justice40-data.s3.amazonaws.com/data-pipeline-staging/${{env.PR_NUMBER}}/${{env.SHA_NUMBER}}/data/score/tiles
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          repo-token-user-login: "github-actions[bot]"
          allow-repeats: false
